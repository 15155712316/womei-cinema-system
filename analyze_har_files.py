#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HARæ–‡ä»¶åˆ†æå·¥å…·
ç”¨äºå¯¹æ¯”åè”å’Œæ²ƒç¾ä¸‹å•ç³»ç»Ÿçš„HTTPè¯·æ±‚å·®å¼‚
"""

import json
import pandas as pd
from urllib.parse import urlparse, parse_qs
import re

def load_har_file(file_path):
    """åŠ è½½HARæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return None

def extract_requests(har_data):
    """æå–HARæ–‡ä»¶ä¸­çš„è¯·æ±‚ä¿¡æ¯"""
    if not har_data or 'log' not in har_data:
        return []
    
    requests = []
    entries = har_data['log'].get('entries', [])
    
    for entry in entries:
        request = entry.get('request', {})
        response = entry.get('response', {})
        
        # è§£æURL
        url = request.get('url', '')
        parsed_url = urlparse(url)
        
        # æå–è¯·æ±‚å‚æ•°
        query_params = {}
        if parsed_url.query:
            query_params = parse_qs(parsed_url.query)
        
        # æå–POSTæ•°æ®
        post_data = {}
        if request.get('postData'):
            post_data_text = request['postData'].get('text', '')
            if post_data_text:
                try:
                    post_data = json.loads(post_data_text)
                except:
                    post_data = {'raw': post_data_text}
        
        # æå–è¯·æ±‚å¤´
        headers = {}
        for header in request.get('headers', []):
            headers[header['name']] = header['value']
        
        request_info = {
            'method': request.get('method', ''),
            'url': url,
            'domain': parsed_url.netloc,
            'path': parsed_url.path,
            'query_params': query_params,
            'post_data': post_data,
            'headers': headers,
            'status_code': response.get('status', 0),
            'response_headers': {h['name']: h['value'] for h in response.get('headers', [])},
            'mime_type': response.get('content', {}).get('mimeType', ''),
            'started_time': entry.get('startedDateTime', ''),
            'time': entry.get('time', 0)
        }
        
        requests.append(request_info)
    
    return requests

def analyze_api_requests(requests):
    """åˆ†æAPIè¯·æ±‚ï¼Œè¿‡æ»¤æ‰é™æ€èµ„æº"""
    api_requests = []
    
    # å®šä¹‰é™æ€èµ„æºçš„ç‰¹å¾
    static_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.ico', '.svg', '.woff', '.ttf']
    static_mime_types = ['image/', 'text/css', 'application/javascript', 'font/']
    
    for req in requests:
        # è·³è¿‡é™æ€èµ„æº
        is_static = False
        
        # æ£€æŸ¥URLæ‰©å±•å
        for ext in static_extensions:
            if req['path'].lower().endswith(ext):
                is_static = True
                break
        
        # æ£€æŸ¥MIMEç±»å‹
        if not is_static:
            for mime in static_mime_types:
                if req['mime_type'].startswith(mime):
                    is_static = True
                    break
        
        if not is_static:
            api_requests.append(req)
    
    return api_requests

def compare_requests(requests1, requests2, label1, label2):
    """å¯¹æ¯”ä¸¤ç»„è¯·æ±‚çš„å·®å¼‚"""
    print(f"\n{'='*60}")
    print(f"APIè¯·æ±‚å¯¹æ¯”åˆ†æ: {label1} vs {label2}")
    print(f"{'='*60}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"{label1}: {len(requests1)} ä¸ªAPIè¯·æ±‚")
    print(f"{label2}: {len(requests2)} ä¸ªAPIè¯·æ±‚")
    
    # åŸŸååˆ†æ
    domains1 = set(req['domain'] for req in requests1)
    domains2 = set(req['domain'] for req in requests2)
    
    print(f"\nğŸŒ åŸŸååˆ†æ:")
    print(f"{label1} ä½¿ç”¨çš„åŸŸå:")
    for domain in sorted(domains1):
        count = sum(1 for req in requests1 if req['domain'] == domain)
        print(f"  - {domain} ({count} ä¸ªè¯·æ±‚)")
    
    print(f"\n{label2} ä½¿ç”¨çš„åŸŸå:")
    for domain in sorted(domains2):
        count = sum(1 for req in requests2 if req['domain'] == domain)
        print(f"  - {domain} ({count} ä¸ªè¯·æ±‚)")
    
    # å…±åŒåŸŸåå’Œç‹¬æœ‰åŸŸå
    common_domains = domains1 & domains2
    unique_domains1 = domains1 - domains2
    unique_domains2 = domains2 - domains1
    
    if common_domains:
        print(f"\nğŸ¤ å…±åŒä½¿ç”¨çš„åŸŸå: {', '.join(sorted(common_domains))}")
    if unique_domains1:
        print(f"\nğŸ”¸ {label1} ç‹¬æœ‰åŸŸå: {', '.join(sorted(unique_domains1))}")
    if unique_domains2:
        print(f"\nğŸ”¹ {label2} ç‹¬æœ‰åŸŸå: {', '.join(sorted(unique_domains2))}")
    
    # APIè·¯å¾„åˆ†æ
    print(f"\nğŸ›£ï¸ APIè·¯å¾„åˆ†æ:")
    paths1 = [req['path'] for req in requests1 if req['method'] in ['POST', 'PUT', 'PATCH']]
    paths2 = [req['path'] for req in requests2 if req['method'] in ['POST', 'PUT', 'PATCH']]
    
    print(f"\n{label1} çš„ä¸»è¦APIè·¯å¾„:")
    for path in sorted(set(paths1)):
        count = paths1.count(path)
        print(f"  - {path} ({count} æ¬¡)")
    
    print(f"\n{label2} çš„ä¸»è¦APIè·¯å¾„:")
    for path in sorted(set(paths2)):
        count = paths2.count(path)
        print(f"  - {path} ({count} æ¬¡)")
    
    # è¯·æ±‚æ–¹æ³•åˆ†æ
    methods1 = [req['method'] for req in requests1]
    methods2 = [req['method'] for req in requests2]
    
    print(f"\nğŸ“¡ è¯·æ±‚æ–¹æ³•ç»Ÿè®¡:")
    print(f"{label1}:")
    for method in sorted(set(methods1)):
        count = methods1.count(method)
        print(f"  - {method}: {count} æ¬¡")
    
    print(f"\n{label2}:")
    for method in sorted(set(methods2)):
        count = methods2.count(method)
        print(f"  - {method}: {count} æ¬¡")

def analyze_request_details(requests, label):
    """è¯¦ç»†åˆ†æè¯·æ±‚å†…å®¹"""
    print(f"\n{'='*60}")
    print(f"è¯¦ç»†è¯·æ±‚åˆ†æ: {label}")
    print(f"{'='*60}")
    
    # æŒ‰åŸŸååˆ†ç»„åˆ†æ
    domain_groups = {}
    for req in requests:
        domain = req['domain']
        if domain not in domain_groups:
            domain_groups[domain] = []
        domain_groups[domain].append(req)
    
    for domain, domain_requests in domain_groups.items():
        print(f"\nğŸ¢ åŸŸå: {domain}")
        print(f"   è¯·æ±‚æ•°é‡: {len(domain_requests)}")
        
        # åˆ†æè¯¥åŸŸåä¸‹çš„API
        api_requests = [req for req in domain_requests if req['method'] in ['POST', 'PUT', 'PATCH']]
        if api_requests:
            print(f"   APIè¯·æ±‚: {len(api_requests)} ä¸ª")
            for req in api_requests:
                print(f"     - {req['method']} {req['path']}")
                if req['post_data']:
                    print(f"       POSTæ•°æ®: {str(req['post_data'])[:100]}...")
                if req['query_params']:
                    print(f"       æŸ¥è¯¢å‚æ•°: {req['query_params']}")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½HARæ–‡ä»¶
    print("æ­£åœ¨åŠ è½½HARæ–‡ä»¶...")
    
    huanlian_har = load_har_file('åè”ä¸‹å•_2025_06_08_15_06_36.har')
    womei_har = load_har_file('æ²ƒç¾ä¸‹å•_2025_06_08_15_07_51.har')
    
    if not huanlian_har or not womei_har:
        print("âŒ æ— æ³•åŠ è½½HARæ–‡ä»¶")
        return
    
    # æå–è¯·æ±‚ä¿¡æ¯
    print("æ­£åœ¨æå–è¯·æ±‚ä¿¡æ¯...")
    huanlian_requests = extract_requests(huanlian_har)
    womei_requests = extract_requests(womei_har)
    
    print(f"åè”ç³»ç»Ÿ: æå–åˆ° {len(huanlian_requests)} ä¸ªè¯·æ±‚")
    print(f"æ²ƒç¾ç³»ç»Ÿ: æå–åˆ° {len(womei_requests)} ä¸ªè¯·æ±‚")
    
    # è¿‡æ»¤APIè¯·æ±‚
    huanlian_api = analyze_api_requests(huanlian_requests)
    womei_api = analyze_api_requests(womei_requests)
    
    print(f"åè”ç³»ç»Ÿ: è¿‡æ»¤å {len(huanlian_api)} ä¸ªAPIè¯·æ±‚")
    print(f"æ²ƒç¾ç³»ç»Ÿ: è¿‡æ»¤å {len(womei_api)} ä¸ªAPIè¯·æ±‚")
    
    # å¯¹æ¯”åˆ†æ
    compare_requests(huanlian_api, womei_api, "åè”ç³»ç»Ÿ", "æ²ƒç¾ç³»ç»Ÿ")
    
    # è¯¦ç»†åˆ†æ
    analyze_request_details(huanlian_api, "åè”ç³»ç»Ÿ")
    analyze_request_details(womei_api, "æ²ƒç¾ç³»ç»Ÿ")
    
    print(f"\n{'='*60}")
    print("åˆ†æå®Œæˆï¼")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
