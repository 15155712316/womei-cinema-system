#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyQt5ç”µå½±ç¥¨åŠ¡ç®¡ç†ç³»ç»Ÿ - ä»£ç é‡å¤å’Œå†—ä½™åˆ†æå™¨
å…¨é¢æ£€æµ‹ä»£ç é‡å¤ã€æ¥å£å†—ä½™å’Œèµ„æºæµªè´¹
"""

import os
import re
import ast
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter
import difflib

class CodeDuplicationAnalyzer:
    """ä»£ç é‡å¤å’Œå†—ä½™åˆ†æå™¨"""
    
    def __init__(self):
        self.project_root = Path(".")
        self.main_file = "main_modular.py"
        self.core_dirs = ["services", "ui", "utils", "modules", "controllers", "views", "widgets"]
        
        self.analysis_results = {
            'duplicate_methods': [],
            'duplicate_code_blocks': [],
            'redundant_imports': [],
            'redundant_apis': [],
            'unused_resources': [],
            'similar_functions': [],
            'optimization_suggestions': [],
            'refactoring_plan': {}
        }
        
        self.code_blocks = {}  # å­˜å‚¨ä»£ç å—çš„å“ˆå¸Œå€¼
        self.method_signatures = {}  # å­˜å‚¨æ–¹æ³•ç­¾å
        self.import_usage = defaultdict(list)  # å¯¼å…¥ä½¿ç”¨æƒ…å†µ
        self.api_usage = defaultdict(int)  # APIä½¿ç”¨é¢‘ç‡
    
    def analyze_main_program_duplicates(self):
        """åˆ†æä¸»ç¨‹åºä¸­çš„é‡å¤ä»£ç """
        print("ğŸ” åˆ†æä¸»ç¨‹åºä»£ç é‡å¤...")
        
        if not Path(self.main_file).exists():
            print(f"âŒ ä¸»ç¨‹åºæ–‡ä»¶ä¸å­˜åœ¨: {self.main_file}")
            return
        
        try:
            with open(self.main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†ææ–¹æ³•é‡å¤
            self._analyze_method_duplicates(content, self.main_file)
            
            # åˆ†æä»£ç å—é‡å¤
            self._analyze_code_block_duplicates(content, self.main_file)
            
            # åˆ†æå¯¼å…¥é‡å¤
            self._analyze_import_duplicates(content, self.main_file)
            
            print(f"âœ… ä¸»ç¨‹åºé‡å¤åˆ†æå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ä¸»ç¨‹åºåˆ†æå¤±è´¥: {e}")
    
    def _analyze_method_duplicates(self, content: str, file_path: str):
        """åˆ†ææ–¹æ³•é‡å¤"""
        # æå–æ‰€æœ‰æ–¹æ³•
        method_pattern = r'def\s+(\w+)\s*\([^)]*\):\s*\n((?:\s{4,}.*\n)*)'
        methods = re.finditer(method_pattern, content, re.MULTILINE)
        
        method_bodies = {}
        for match in methods:
            method_name = match.group(1)
            method_body = match.group(2)
            
            # æ ‡å‡†åŒ–æ–¹æ³•ä½“ï¼ˆå»é™¤ç©ºç™½å’Œæ³¨é‡Šï¼‰
            normalized_body = self._normalize_code(method_body)
            body_hash = hashlib.md5(normalized_body.encode()).hexdigest()
            
            if body_hash in method_bodies:
                # å‘ç°é‡å¤æ–¹æ³•
                self.analysis_results['duplicate_methods'].append({
                    'file1': method_bodies[body_hash]['file'],
                    'method1': method_bodies[body_hash]['name'],
                    'file2': file_path,
                    'method2': method_name,
                    'similarity': 100,
                    'body_hash': body_hash,
                    'lines': len(method_body.split('\n'))
                })
            else:
                method_bodies[body_hash] = {
                    'file': file_path,
                    'name': method_name,
                    'body': method_body
                }
        
        # æ£€æŸ¥ç›¸ä¼¼æ–¹æ³•ï¼ˆä¸å®Œå…¨ç›¸åŒä½†é«˜åº¦ç›¸ä¼¼ï¼‰
        self._find_similar_methods(method_bodies, file_path)
    
    def _analyze_code_block_duplicates(self, content: str, file_path: str):
        """åˆ†æä»£ç å—é‡å¤"""
        lines = content.split('\n')
        
        # åˆ†æè¿ç»­çš„ä»£ç å—ï¼ˆ5è¡Œä»¥ä¸Šï¼‰
        for i in range(len(lines) - 5):
            block = '\n'.join(lines[i:i+5])
            normalized_block = self._normalize_code(block)
            
            if len(normalized_block.strip()) < 50:  # è·³è¿‡å¤ªçŸ­çš„å—
                continue
            
            block_hash = hashlib.md5(normalized_block.encode()).hexdigest()
            
            if block_hash in self.code_blocks:
                self.analysis_results['duplicate_code_blocks'].append({
                    'file1': self.code_blocks[block_hash]['file'],
                    'lines1': self.code_blocks[block_hash]['lines'],
                    'file2': file_path,
                    'lines2': f"{i+1}-{i+5}",
                    'block_hash': block_hash,
                    'content': block[:100] + "..." if len(block) > 100 else block
                })
            else:
                self.code_blocks[block_hash] = {
                    'file': file_path,
                    'lines': f"{i+1}-{i+5}",
                    'content': block
                }
    
    def _analyze_import_duplicates(self, content: str, file_path: str):
        """åˆ†æå¯¼å…¥é‡å¤"""
        import_pattern = r'^(from\s+[\w.]+\s+import\s+[\w,\s*]+|import\s+[\w.,\s]+)'
        imports = re.findall(import_pattern, content, re.MULTILINE)
        
        import_counts = Counter(imports)
        for imp, count in import_counts.items():
            if count > 1:
                self.analysis_results['redundant_imports'].append({
                    'file': file_path,
                    'import': imp,
                    'count': count
                })
    
    def _normalize_code(self, code: str) -> str:
        """æ ‡å‡†åŒ–ä»£ç ï¼ˆå»é™¤ç©ºç™½ã€æ³¨é‡Šç­‰ï¼‰"""
        # å»é™¤æ³¨é‡Š
        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)
        # å»é™¤å¤šä½™ç©ºç™½
        code = re.sub(r'\s+', ' ', code)
        # å»é™¤ç©ºè¡Œ
        code = '\n'.join(line.strip() for line in code.split('\n') if line.strip())
        return code.strip()
    
    def _find_similar_methods(self, method_bodies: Dict, file_path: str):
        """æŸ¥æ‰¾ç›¸ä¼¼æ–¹æ³•"""
        bodies_list = list(method_bodies.values())
        
        for i, method1 in enumerate(bodies_list):
            for j, method2 in enumerate(bodies_list[i+1:], i+1):
                similarity = self._calculate_similarity(method1['body'], method2['body'])
                
                if similarity > 70:  # ç›¸ä¼¼åº¦è¶…è¿‡70%
                    self.analysis_results['similar_functions'].append({
                        'file1': method1['file'],
                        'method1': method1['name'],
                        'file2': method2['file'],
                        'method2': method2['name'],
                        'similarity': similarity
                    })
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        normalized1 = self._normalize_code(text1)
        normalized2 = self._normalize_code(text2)
        
        matcher = difflib.SequenceMatcher(None, normalized1, normalized2)
        return matcher.ratio() * 100
    
    def analyze_directory_duplicates(self):
        """åˆ†æç›®å½•ä¸­çš„é‡å¤ä»£ç """
        print("ğŸ” åˆ†æç›®å½•ä»£ç é‡å¤...")
        
        for dir_name in self.core_dirs:
            dir_path = Path(dir_name)
            if dir_path.exists():
                self._analyze_directory(dir_path)
                print(f"âœ… åˆ†æå®Œæˆ: {dir_name}")
    
    def _analyze_directory(self, dir_path: Path):
        """åˆ†æå•ä¸ªç›®å½•"""
        for file_path in dir_path.rglob("*.py"):
            if file_path.name == "__init__.py":
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                self._analyze_method_duplicates(content, str(file_path))
                self._analyze_code_block_duplicates(content, str(file_path))
                self._analyze_import_duplicates(content, str(file_path))
                self._analyze_api_usage(content, str(file_path))
                
            except Exception as e:
                print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    def _analyze_api_usage(self, content: str, file_path: str):
        """åˆ†æAPIä½¿ç”¨æƒ…å†µ"""
        # æŸ¥æ‰¾APIè°ƒç”¨æ¨¡å¼
        api_patterns = [
            r'def\s+(api_\w+)',  # APIæ–¹æ³•å®šä¹‰
            r'(\w+\.api_\w+)',   # APIæ–¹æ³•è°ƒç”¨
            r'(requests\.\w+)',  # requestsè°ƒç”¨
            r'(\w+_api\(\w*\))'  # APIå‡½æ•°è°ƒç”¨
        ]
        
        for pattern in api_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                self.api_usage[match] += 1
    
    def analyze_redundant_apis(self):
        """åˆ†æå†—ä½™API"""
        print("ğŸ” åˆ†æå†—ä½™APIæ¥å£...")
        
        # æŸ¥æ‰¾ä½¿ç”¨é¢‘ç‡ä½çš„API
        low_usage_apis = {api: count for api, count in self.api_usage.items() 
                         if count <= 1 and 'api' in api.lower()}
        
        # æŸ¥æ‰¾åŠŸèƒ½é‡å çš„API
        api_groups = self._group_similar_apis()
        
        self.analysis_results['redundant_apis'] = {
            'low_usage': low_usage_apis,
            'similar_groups': api_groups
        }
        
        print(f"âœ… å‘ç° {len(low_usage_apis)} ä¸ªä½ä½¿ç”¨API")
    
    def _group_similar_apis(self) -> List[List[str]]:
        """å°†ç›¸ä¼¼çš„APIåˆ†ç»„"""
        apis = [api for api in self.api_usage.keys() if 'api' in api.lower()]
        groups = []
        
        for api in apis:
            # ç®€å•çš„ç›¸ä¼¼æ€§æ£€æŸ¥ï¼ˆåŸºäºåç§°ï¼‰
            similar_apis = [a for a in apis if a != api and 
                          self._api_name_similarity(api, a) > 0.7]
            
            if similar_apis:
                group = [api] + similar_apis
                if group not in groups:
                    groups.append(group)
        
        return groups
    
    def _api_name_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—APIåç§°ç›¸ä¼¼åº¦"""
        # æå–å…³é”®è¯
        words1 = set(re.findall(r'\w+', name1.lower()))
        words2 = set(re.findall(r'\w+', name2.lower()))
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def analyze_unused_resources(self):
        """åˆ†ææœªä½¿ç”¨çš„èµ„æº"""
        print("ğŸ” åˆ†ææœªä½¿ç”¨èµ„æº...")
        
        # åˆ†ææœªä½¿ç”¨çš„å¯¼å…¥
        self._find_unused_imports()
        
        # åˆ†ææœªä½¿ç”¨çš„æ–¹æ³•
        self._find_unused_methods()
        
        print("âœ… æœªä½¿ç”¨èµ„æºåˆ†æå®Œæˆ")
    
    def _find_unused_imports(self):
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„å¯¼å…¥"""
        for dir_name in self.core_dirs:
            dir_path = Path(dir_name)
            if not dir_path.exists():
                continue
            
            for file_path in dir_path.rglob("*.py"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æå–å¯¼å…¥
                    imports = re.findall(r'from\s+([\w.]+)\s+import\s+([\w,\s*]+)', content)
                    imports.extend(re.findall(r'import\s+([\w.,\s]+)', content))
                    
                    # æ£€æŸ¥ä½¿ç”¨æƒ…å†µ
                    for imp in imports:
                        if isinstance(imp, tuple):
                            module, items = imp
                            for item in items.split(','):
                                item = item.strip()
                                if item and item not in content[content.find(f'import {item}') + 20:]:
                                    self.analysis_results['unused_resources'].append({
                                        'type': 'import',
                                        'file': str(file_path),
                                        'resource': f"{module}.{item}",
                                        'reason': 'imported but not used'
                                    })
                        else:
                            module = imp.strip()
                            if module and module not in content[content.find(f'import {module}') + 20:]:
                                self.analysis_results['unused_resources'].append({
                                    'type': 'import',
                                    'file': str(file_path),
                                    'resource': module,
                                    'reason': 'imported but not used'
                                })
                
                except Exception:
                    continue
    
    def _find_unused_methods(self):
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„æ–¹æ³•"""
        # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…é¡¹ç›®ä¸­éœ€è¦æ›´å¤æ‚çš„åˆ†æ
        all_methods = set()
        method_calls = set()
        
        # æ”¶é›†æ‰€æœ‰æ–¹æ³•å®šä¹‰å’Œè°ƒç”¨
        for dir_name in self.core_dirs + ['.']:
            dir_path = Path(dir_name)
            if not dir_path.exists():
                continue
            
            for file_path in dir_path.rglob("*.py"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ–¹æ³•å®šä¹‰
                    methods = re.findall(r'def\s+(\w+)', content)
                    all_methods.update(methods)
                    
                    # æ–¹æ³•è°ƒç”¨
                    calls = re.findall(r'(\w+)\s*\(', content)
                    method_calls.update(calls)
                
                except Exception:
                    continue
        
        # æ‰¾å‡ºæœªè¢«è°ƒç”¨çš„æ–¹æ³•
        unused_methods = all_methods - method_calls
        for method in unused_methods:
            if not method.startswith('_'):  # è·³è¿‡ç§æœ‰æ–¹æ³•
                self.analysis_results['unused_resources'].append({
                    'type': 'method',
                    'resource': method,
                    'reason': 'defined but not called'
                })
    
    def generate_optimization_suggestions(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("ğŸ” ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        suggestions = []
        
        # é‡å¤æ–¹æ³•ä¼˜åŒ–
        if self.analysis_results['duplicate_methods']:
            suggestions.append({
                'type': 'duplicate_methods',
                'priority': 'high',
                'description': f"å‘ç° {len(self.analysis_results['duplicate_methods'])} ä¸ªé‡å¤æ–¹æ³•",
                'action': 'æå–å…¬å…±æ–¹æ³•åˆ°åŸºç±»æˆ–å·¥å…·æ¨¡å—',
                'estimated_reduction': f"{sum(d['lines'] for d in self.analysis_results['duplicate_methods'])} è¡Œä»£ç "
            })
        
        # é‡å¤ä»£ç å—ä¼˜åŒ–
        if self.analysis_results['duplicate_code_blocks']:
            suggestions.append({
                'type': 'duplicate_blocks',
                'priority': 'medium',
                'description': f"å‘ç° {len(self.analysis_results['duplicate_code_blocks'])} ä¸ªé‡å¤ä»£ç å—",
                'action': 'æå–ä¸ºå…¬å…±å‡½æ•°æˆ–å¸¸é‡',
                'estimated_reduction': f"{len(self.analysis_results['duplicate_code_blocks']) * 5} è¡Œä»£ç "
            })
        
        # å†—ä½™å¯¼å…¥ä¼˜åŒ–
        if self.analysis_results['redundant_imports']:
            suggestions.append({
                'type': 'redundant_imports',
                'priority': 'low',
                'description': f"å‘ç° {len(self.analysis_results['redundant_imports'])} ä¸ªå†—ä½™å¯¼å…¥",
                'action': 'ç§»é™¤é‡å¤çš„å¯¼å…¥è¯­å¥',
                'estimated_reduction': f"{len(self.analysis_results['redundant_imports'])} è¡Œä»£ç "
            })
        
        # æœªä½¿ç”¨èµ„æºä¼˜åŒ–
        if self.analysis_results['unused_resources']:
            suggestions.append({
                'type': 'unused_resources',
                'priority': 'medium',
                'description': f"å‘ç° {len(self.analysis_results['unused_resources'])} ä¸ªæœªä½¿ç”¨èµ„æº",
                'action': 'ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥å’Œæ–¹æ³•',
                'estimated_reduction': f"{len(self.analysis_results['unused_resources'])} ä¸ªèµ„æº"
            })
        
        self.analysis_results['optimization_suggestions'] = suggestions
        print(f"âœ… ç”Ÿæˆ {len(suggestions)} ä¸ªä¼˜åŒ–å»ºè®®")
    
    def generate_refactoring_plan(self):
        """ç”Ÿæˆé‡æ„è®¡åˆ’"""
        print("ğŸ” ç”Ÿæˆé‡æ„è®¡åˆ’...")
        
        plan = {
            'phase_1': {
                'name': 'æ¸…ç†æœªä½¿ç”¨èµ„æº',
                'duration': '1-2å¤©',
                'risk': 'low',
                'tasks': [
                    'ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥è¯­å¥',
                    'åˆ é™¤æœªè°ƒç”¨çš„æ–¹æ³•',
                    'æ¸…ç†å†—ä½™çš„å˜é‡å®šä¹‰'
                ]
            },
            'phase_2': {
                'name': 'åˆå¹¶é‡å¤ä»£ç å—',
                'duration': '2-3å¤©',
                'risk': 'medium',
                'tasks': [
                    'æå–é‡å¤çš„ä»£ç å—ä¸ºå‡½æ•°',
                    'åˆ›å»ºå…¬å…±å¸¸é‡æ–‡ä»¶',
                    'ç»Ÿä¸€é”™è¯¯å¤„ç†é€»è¾‘'
                ]
            },
            'phase_3': {
                'name': 'é‡æ„é‡å¤æ–¹æ³•',
                'duration': '3-5å¤©',
                'risk': 'high',
                'tasks': [
                    'æå–å…¬å…±æ–¹æ³•åˆ°åŸºç±»',
                    'åˆ›å»ºå·¥å…·ç±»å’Œè¾…åŠ©å‡½æ•°',
                    'é‡æ„ç›¸ä¼¼çš„APIæ¥å£'
                ]
            },
            'phase_4': {
                'name': 'ä¼˜åŒ–APIæ¥å£',
                'duration': '2-3å¤©',
                'risk': 'medium',
                'tasks': [
                    'åˆå¹¶åŠŸèƒ½é‡å çš„API',
                    'ç®€åŒ–APIè°ƒç”¨é“¾è·¯',
                    'ä¼˜åŒ–APIå‚æ•°è®¾è®¡'
                ]
            }
        }
        
        self.analysis_results['refactoring_plan'] = plan
        print("âœ… é‡æ„è®¡åˆ’ç”Ÿæˆå®Œæˆ")
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆä»£ç é‡å¤åˆ†ææŠ¥å‘Š...")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_duplicates = (len(self.analysis_results['duplicate_methods']) + 
                          len(self.analysis_results['duplicate_code_blocks']))
        
        total_redundant = (len(self.analysis_results['redundant_imports']) + 
                         len(self.analysis_results['unused_resources']))
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            'analysis_date': '2025-06-06',
            'summary': {
                'total_duplicates': total_duplicates,
                'total_redundant': total_redundant,
                'optimization_potential': 'high' if total_duplicates > 10 else 'medium'
            },
            'analysis_results': self.analysis_results
        }
        
        with open('code_duplication_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ¬ PyQt5ç”µå½±ç¥¨åŠ¡ç®¡ç†ç³»ç»Ÿ - ä»£ç é‡å¤åˆ†æ")
        print("=" * 60)
        
        self.analyze_main_program_duplicates()
        self.analyze_directory_duplicates()
        self.analyze_redundant_apis()
        self.analyze_unused_resources()
        self.generate_optimization_suggestions()
        self.generate_refactoring_plan()
        
        return self.generate_report()

def main():
    """ä¸»å‡½æ•°"""
    analyzer = CodeDuplicationAnalyzer()
    results = analyzer.run_analysis()
    
    # æ˜¾ç¤ºæ‘˜è¦
    summary = results['summary']
    print(f"\nğŸ“Š åˆ†ææ‘˜è¦:")
    print(f"  å‘ç°é‡å¤é¡¹: {summary['total_duplicates']}")
    print(f"  å‘ç°å†—ä½™é¡¹: {summary['total_redundant']}")
    print(f"  ä¼˜åŒ–æ½œåŠ›: {summary['optimization_potential']}")
    
    print(f"\nâœ… ä»£ç é‡å¤åˆ†æå®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: code_duplication_analysis.json")

if __name__ == "__main__":
    main()
